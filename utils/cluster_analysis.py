import pandas as pd
import numpy as np
from sklearn.cluster import KMeans, AgglomerativeClustering
from sklearn.mixture import GaussianMixture
import logging

def analyze_cluster_characteristics(X, labels, feature_names, original_data=None, top_features=10):
    """
    PhÃ¢n tÃ­ch Ä‘áº·c trÆ°ng cá»§a tá»«ng cá»¥m
    
    Args:
        X: Dá»¯ liá»‡u Ä‘Ã£ chuáº©n hÃ³a (numpy array hoáº·c DataFrame)
        labels: NhÃ£n cá»¥m cho tá»«ng Ä‘iá»ƒm dá»¯ liá»‡u
        feature_names: TÃªn cÃ¡c features (cho dá»¯ liá»‡u clustering)
        original_data: Dá»¯ liá»‡u gá»‘c chÆ°a chuáº©n hÃ³a (Ä‘á»ƒ hiá»ƒn thá»‹ giÃ¡ trá»‹ thá»±c)
        top_features: Sá»‘ lÆ°á»£ng features Ä‘áº·c trÆ°ng nháº¥t Ä‘á»ƒ hiá»ƒn thá»‹
    
    Returns:
        dict: ThÃ´ng tin Ä‘áº·c trÆ°ng cá»§a tá»«ng cá»¥m
    """
    try:
        # Kiá»ƒm tra dá»¯ liá»‡u Ä‘áº§u vÃ o
        logging.info(f"analyze_cluster_characteristics - Input shapes: X={X.shape if hasattr(X, 'shape') else 'unknown'}, "
                      f"labels={len(labels) if labels is not None else 'None'}, feature_names={len(feature_names)}")
                 
        # Kiá»ƒm tra NaN
        if isinstance(X, np.ndarray) and np.isnan(X).any():
            logging.warning("NaN values found in X, replacing with zeros")
            X = np.nan_to_num(X, nan=0.0)
        
        # Chuyá»ƒn Ä‘á»•i vá» DataFrame náº¿u cáº§n (cho clustering data)
        if isinstance(X, np.ndarray):
            if X.shape[1] != len(feature_names):
                logging.warning(f"Feature names count ({len(feature_names)}) doesn't match X columns ({X.shape[1]}). Using generic names.")
                feature_names = [f'Feature_{i}' for i in range(X.shape[1])]
            df = pd.DataFrame(X, columns=feature_names)
        else:
            df = X.copy()
            
        # ThÃªm cá»™t cluster cho clustering data
        df['cluster'] = labels
        
        # Chuáº©n bá»‹ dá»¯ liá»‡u gá»‘c
        if original_data is not None:
            if isinstance(original_data, np.ndarray):
                # Táº¡o DataFrame tá»« original_data
                original_feature_names = [f'Original_Feature_{i}' for i in range(original_data.shape[1])]
                original_df = pd.DataFrame(original_data, columns=original_feature_names)
            else:
                original_df = original_data.copy()
                
            # Kiá»ƒm tra NaN
            if original_df.isna().any().any():
                logging.warning("NaN values found in original_data, filling with column means")
                original_df = original_df.fillna(original_df.mean())
                
            # ThÃªm cá»™t cluster
            if len(original_df) == len(labels):
                original_df['cluster'] = labels
                logging.info(f"Added cluster labels to original data. Shape: {original_df.shape}")
            else:
                logging.warning(f"Length mismatch: original_df ({len(original_df)}) vs labels ({len(labels)}). Using clustering data instead.")
                original_df = df.copy()
        else:
            original_df = df.copy()
            logging.info("No original data provided, using clustering data")
        
        cluster_analysis = {}
        unique_clusters = sorted(df['cluster'].unique())
        
        # Láº¥y danh sÃ¡ch feature names tá»« original data (bá» qua cá»™t cluster)
        analysis_feature_names = [col for col in original_df.columns if col != 'cluster']
        logging.info(f"Analysis feature names: {analysis_feature_names[:5]}... (total: {len(analysis_feature_names)})")
        
        for cluster_id in unique_clusters:
            cluster_data = df[df['cluster'] == cluster_id]
            original_cluster_data = original_df[original_df['cluster'] == cluster_id]
            
            cluster_size = len(cluster_data)
            cluster_percentage = (cluster_size / len(df)) * 100
            
            # TÃ­nh trung bÃ¬nh tá»« dá»¯ liá»‡u Ä‘Ã£ chuáº©n hÃ³a (Ä‘á»ƒ xáº¿p háº¡ng)
            cluster_means = cluster_data[feature_names].mean()
            overall_means = df[feature_names].mean()
            mean_differences = cluster_means - overall_means
            
            # TÃ­nh trung bÃ¬nh tá»« dá»¯ liá»‡u gá»‘c thá»±c táº¿ (Ä‘á»ƒ hiá»ƒn thá»‹)
            original_cluster_means = original_cluster_data[analysis_feature_names].mean()
            original_overall_means = original_df[analysis_feature_names].mean()
            
            # TÃ¬m features Ä‘áº·c trÆ°ng (cÃ³ Ä‘á»™ lá»‡ch lá»›n nháº¥t)
            distinctive_features = []
            
            # Sáº¯p xáº¿p theo Ä‘á»™ lá»‡ch tuyá»‡t Ä‘á»‘i tá»« normalized data
            sorted_features = mean_differences.abs().sort_values(ascending=False)
            
            # Láº¥y top features tá»« normalized data nhÆ°ng hiá»ƒn thá»‹ báº±ng original data
            for i, norm_feature in enumerate(sorted_features.head(top_features).index):
                if i < len(analysis_feature_names):
                    # Láº¥y tÃªn feature gá»‘c tÆ°Æ¡ng á»©ng
                    original_feature_name = analysis_feature_names[i]
                    
                    # Láº¥y giÃ¡ trá»‹ tá»« dá»¯ liá»‡u gá»‘c
                    cluster_mean_original = original_cluster_means[original_feature_name]
                    overall_mean_original = original_overall_means[original_feature_name]
                    
                    # TÃ­nh difference tá»« normalized data Ä‘á»ƒ classify significance
                    difference_normalized = mean_differences[norm_feature]
                    
                    # PhÃ¢n loáº¡i má»©c Ä‘á»™ Ä‘áº·c trÆ°ng dá»±a trÃªn normalized difference
                    if abs(difference_normalized) > 2:
                        significance = "Ráº¥t cao"
                    elif abs(difference_normalized) > 1:
                        significance = "Cao"
                    elif abs(difference_normalized) > 0.5:
                        significance = "Trung bÃ¬nh"
                    else:
                        significance = "Tháº¥p"
                    
                    # XÃ¡c Ä‘á»‹nh xu hÆ°á»›ng
                    if difference_normalized > 0:
                        trend = "cao hÆ¡n"
                        trend_icon = "ğŸ“ˆ"
                    else:
                        trend = "tháº¥p hÆ¡n"
                        trend_icon = "ğŸ“‰"
                    
                    # TÃ­nh sá»‘ lÆ°á»£ng pháº§n tá»­ cÃ³ giÃ¡ trá»‹ cao/tháº¥p
                    threshold = overall_mean_original
                    if difference_normalized > 0:
                        count_above_threshold = (original_cluster_data[original_feature_name] > threshold).sum()
                        percentage_above = (count_above_threshold / cluster_size) * 100
                        description = f"{count_above_threshold}/{cluster_size} pháº§n tá»­ ({percentage_above:.1f}%) cÃ³ {original_feature_name} > {threshold:.2f}"
                    else:
                        count_below_threshold = (original_cluster_data[original_feature_name] < threshold).sum()
                        percentage_below = (count_below_threshold / cluster_size) * 100
                        description = f"{count_below_threshold}/{cluster_size} pháº§n tá»­ ({percentage_below:.1f}%) cÃ³ {original_feature_name} < {threshold:.2f}"
                    
                    # TÃ­nh percentile
                    percentile = (original_cluster_data[original_feature_name] > overall_mean_original).mean() * 100
                    
                    distinctive_features.append({
                        'feature': original_feature_name,  # Hiá»ƒn thá»‹ tÃªn feature gá»‘c
                        'cluster_mean': cluster_mean_original,  # GiÃ¡ trá»‹ gá»‘c
                        'overall_mean': overall_mean_original,  # GiÃ¡ trá»‹ gá»‘c
                        'difference': difference_normalized,  # Normalized difference Ä‘á»ƒ sort
                        'abs_difference': abs(difference_normalized),
                        'trend': trend,
                        'trend_icon': trend_icon,
                        'significance': significance,
                        'percentile': percentile,
                        'description': description
                    })
            
            # TÃ¬m features cÃ³ phÆ°Æ¡ng sai tháº¥p nháº¥t trong cá»¥m (á»•n Ä‘á»‹nh)
            cluster_stds = cluster_data[feature_names].std()
            most_stable_features = cluster_stds.sort_values().head(5)
            
            stable_features = []
            for j, feature in enumerate(most_stable_features.index):
                if j < len(analysis_feature_names):
                    original_feature_name = analysis_feature_names[j]
                    stable_features.append({
                        'feature': original_feature_name,
                        'std': cluster_stds[feature],
                        'mean': original_cluster_means[original_feature_name],
                        'description': f"{original_feature_name} ráº¥t á»•n Ä‘á»‹nh (std: {cluster_stds[feature]:.3f})"
                    })
            
            # Táº¡o mÃ´ táº£ tá»•ng quan cho cá»¥m
            top_3_features = distinctive_features[:3]
            if top_3_features:
                summary_parts = []
                for feat in top_3_features:
                    summary_parts.append(f"{feat['feature']} {feat['trend']} trung bÃ¬nh")
                summary = f"Cá»¥m Ä‘áº·c trÆ°ng bá»Ÿi: " + ", ".join(summary_parts)
            else:
                summary = "Cá»¥m cÃ³ Ä‘áº·c Ä‘iá»ƒm trung bÃ¬nh, khÃ´ng cÃ³ features ná»•i báº­t Ä‘áº·c biá»‡t"
            
            cluster_analysis[cluster_id] = {
                'cluster_id': cluster_id,
                'size': cluster_size,
                'percentage': cluster_percentage,
                'summary': summary,
                'distinctive_features': distinctive_features,
                'stable_features': stable_features,
                'cluster_means': original_cluster_means.to_dict(),
                'statistics': {
                    'min_values': original_cluster_data[analysis_feature_names].min().to_dict(),
                    'max_values': original_cluster_data[analysis_feature_names].max().to_dict(),
                    'std_values': original_cluster_data[analysis_feature_names].std().to_dict()
                }
            }
        
        logging.info(f"Successfully analyzed {len(cluster_analysis)} clusters")
        return cluster_analysis
    
    except Exception as e:
        logging.error(f"Error in analyze_cluster_characteristics: {str(e)}")
        logging.error(f"Input details: X shape = {X.shape if hasattr(X, 'shape') else 'Unknown'}, "
                     f"labels count = {len(labels) if labels is not None else 'Unknown'}, "
                     f"feature_names = {feature_names[:5]}...(total: {len(feature_names)}), "
                     f"original_data = {original_data.shape if hasattr(original_data, 'shape') else 'None'}, "
                     f"top_features = {top_features}")
        return {}

def get_cluster_predictions(X, model_name, k, random_state=42):
    """
    Thá»±c hiá»‡n phÃ¢n cá»¥m vá»›i sá»‘ cá»¥m k cho mÃ´ hÃ¬nh cá»¥ thá»ƒ
    
    Args:
        X: Dá»¯ liá»‡u Ä‘á»ƒ phÃ¢n cá»¥m
        model_name: TÃªn mÃ´ hÃ¬nh ('KMeans', 'GMM', 'Hierarchical', 'FuzzyCMeans')
        k: Sá»‘ cá»¥m
        random_state: Seed cho reproducibility
    
    Returns:
        numpy array: NhÃ£n cá»¥m cho tá»«ng Ä‘iá»ƒm dá»¯ liá»‡u
    """
    try:
        if model_name == 'KMeans':
            model = KMeans(n_clusters=k, random_state=random_state, n_init=10)
            labels = model.fit_predict(X)
            
        elif model_name == 'GMM':
            model = GaussianMixture(n_components=k, random_state=random_state)
            labels = model.fit_predict(X)
            
        elif model_name == 'Hierarchical':
            model = AgglomerativeClustering(n_clusters=k)
            labels = model.fit_predict(X)
            
        elif model_name == 'FuzzyCMeans':
            # Fuzzy C-means cáº§n Ä‘Æ°á»£c import riÃªng
            try:
                import skfuzzy as fuzz
                cntr, u, u0, d, jm, p, fpc = fuzz.cluster.cmeans(
                    X.T, k, 2, error=0.005, maxiter=1000, init=None
                )
                labels = np.argmax(u, axis=0)
            except ImportError:
                logging.warning("skfuzzy not available, using KMeans instead")
                model = KMeans(n_clusters=k, random_state=random_state, n_init=10)
                labels = model.fit_predict(X)
        
        else:
            raise ValueError(f"Unknown model: {model_name}")
            
        return labels
        
    except Exception as e:
        logging.error(f"Error in get_cluster_predictions: {str(e)}")
        # Fallback to KMeans
        model = KMeans(n_clusters=k, random_state=random_state, n_init=10)
        return model.fit_predict(X)

def create_cluster_comparison_table(cluster_analysis, feature_names):
    """
    Táº¡o báº£ng so sÃ¡nh Ä‘áº·c trÆ°ng giá»¯a cÃ¡c cá»¥m
    
    Args:
        cluster_analysis: Káº¿t quáº£ phÃ¢n tÃ­ch tá»« analyze_cluster_characteristics
        feature_names: Danh sÃ¡ch tÃªn features
    
    Returns:
        pandas.DataFrame: Báº£ng so sÃ¡nh
    """
    try:
        comparison_data = []
        
        for feature in feature_names:
            row = {'Feature': feature}
            
            # Kiá»ƒm tra xem feature cÃ³ tá»“n táº¡i trong cluster_means khÃ´ng
            cluster_values = {}
            for cluster_id in sorted(cluster_analysis.keys()):
                if feature in cluster_analysis[cluster_id]['cluster_means']:
                    cluster_mean = cluster_analysis[cluster_id]['cluster_means'][feature]
                    row[f'Cá»¥m {cluster_id}'] = f"{cluster_mean:.2f}"
                    cluster_values[cluster_id] = cluster_mean
                else:
                    logging.warning(f"Feature {feature} not found in cluster {cluster_id} means")
            
            # TÃ¬m cá»¥m cÃ³ giÃ¡ trá»‹ cao nháº¥t vÃ  tháº¥p nháº¥t náº¿u cÃ³ dá»¯ liá»‡u
            if cluster_values:
                max_cluster = max(cluster_values, key=cluster_values.get)
                min_cluster = min(cluster_values, key=cluster_values.get)
                
                row['Cao nháº¥t'] = f"Cá»¥m {max_cluster}"
                row['Tháº¥p nháº¥t'] = f"Cá»¥m {min_cluster}"
                row['ChÃªnh lá»‡ch'] = f"{cluster_values[max_cluster] - cluster_values[min_cluster]:.2f}"
                
                comparison_data.append(row)
        
        return pd.DataFrame(comparison_data)
        
    except Exception as e:
        logging.error(f"Error in create_cluster_comparison_table: {str(e)}")
        return pd.DataFrame()
